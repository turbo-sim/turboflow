<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>turboflow.pysolver_view.optimization &mdash; turboflow v0.1.15 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css?v=13237357" />
      <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=87e54e7c" />
      <link rel="stylesheet" type="text/css" href="../../../_static/custom.css?v=abecb913" />

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../../../_static/documentation_options.js?v=49d9c938"></script>
        <script src="../../../_static/doctools.js?v=9a2dae69"></script>
        <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script>let toggleHintShow = 'Click to show';</script>
        <script>let toggleHintHide = 'Click to hide';</script>
        <script>let toggleOpenOnPrint = 'true';</script>
        <script src="../../../_static/togglebutton.js?v=4a39c7ea"></script>
        <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
        <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
        <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../index.html" class="icon icon-home">
            turboflow
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../source/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../source/tutorials.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../source/model_description.html">Model Description</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../source/nomenclature.html">Nomenclature</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../source/glossary.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../source/bibliography.html">Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../source/api/turboflow.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../source/developer_guide.html">Developer guide</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">turboflow</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">turboflow.pysolver_view.optimization</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for turboflow.pysolver_view.optimization</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>
<span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABC</span><span class="p">,</span> <span class="n">abstractmethod</span>
<span class="kn">from</span> <span class="nn">matplotlib.ticker</span> <span class="kn">import</span> <span class="n">MaxNLocator</span>

<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">numerical_differentiation</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">optimization_wrappers</span> <span class="k">as</span> <span class="n">_opt</span>
<span class="kn">from</span> <span class="nn">.pysolver_utilities</span> <span class="kn">import</span> <span class="n">savefig_in_formats</span>


<span class="c1"># Define valid libraries and their corresponding methods</span>
<span class="n">OPTIMIZATION_LIBRARIES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;scipy&quot;</span><span class="p">:</span> <span class="n">_opt</span><span class="o">.</span><span class="n">minimize_scipy</span><span class="p">,</span>
    <span class="s2">&quot;pygmo&quot;</span><span class="p">:</span> <span class="n">_opt</span><span class="o">.</span><span class="n">minimize_pygmo</span><span class="p">,</span>
    <span class="s2">&quot;pygmo_nlopt&quot;</span><span class="p">:</span> <span class="n">_opt</span><span class="o">.</span><span class="n">minimize_nlopt</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">VALID_LIBRARIES_AND_METHODS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;scipy&quot;</span><span class="p">:</span> <span class="n">_opt</span><span class="o">.</span><span class="n">SCIPY_SOLVERS</span><span class="p">,</span>
    <span class="s2">&quot;pygmo&quot;</span><span class="p">:</span> <span class="n">_opt</span><span class="o">.</span><span class="n">PYGMO_SOLVERS</span><span class="p">,</span>
    <span class="s2">&quot;pygmo_nlopt&quot;</span><span class="p">:</span> <span class="n">_opt</span><span class="o">.</span><span class="n">NLOPT_SOLVERS</span><span class="p">,</span>
<span class="p">}</span>


<div class="viewcode-block" id="OptimizationSolver">
<a class="viewcode-back" href="../../../source/api/turboflow.pysolver_view.optimization.html#turboflow.pysolver_view.optimization.OptimizationSolver">[docs]</a>
<span class="k">class</span> <span class="nc">OptimizationSolver</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">    Solver class for general nonlinear programming problems.</span>

<span class="sd">    The solver is designed to handle constrained optimization problems of the form:</span>

<span class="sd">    Minimize:</span>

<span class="sd">    .. math::</span>
<span class="sd">        f(\mathbf{x}) \; \mathrm{with} \; \mathbf{x} \in \mathbb{R}^n</span>

<span class="sd">    Subject to:</span>

<span class="sd">    .. math::</span>
<span class="sd">        c_{\mathrm{eq}}(\mathbf{x}) = 0</span>
<span class="sd">    .. math::</span>
<span class="sd">        c_{\mathrm{in}}(\mathbf{x}) \leq 0</span>
<span class="sd">    .. math::</span>
<span class="sd">        \mathbf{x}_l \leq \mathbf{x} \leq \mathbf{x}_u</span>

<span class="sd">    where:</span>

<span class="sd">    - :math:`\mathbf{x}` is the vector of decision variables (i.e., degree of freedom).</span>
<span class="sd">    - :math:`f(\mathbf{x})` is the objective function to be minimized. Maximization problems can be casted into minimization problems by changing the sign of the objective function.</span>
<span class="sd">    - :math:`c_{\mathrm{eq}}(\mathbf{x})` are the equality constraints of the problem.</span>
<span class="sd">    - :math:`c_{\mathrm{in}}(\mathbf{x})` are the inequality constraints of the problem. Constraints of type :math:`c_{\mathrm{in}}(\mathbf{x}) \leq 0` can be casted into :math:`c_{\mathrm{in}}(\mathbf{x}) \geq 0` type by changing the sign of the constraint functions.</span>
<span class="sd">    - :math:`\mathbf{x}_l` and :math:`\mathbf{x}_u` are the lower and upper bounds on the decision variables.</span>

<span class="sd">    The class interfaces with various optimization methods provided by libraries such as `scipy` and `pygmo` to solve the problem and provides a structured framework for initialization, solution monitoring, and post-processing.</span>

<span class="sd">    This class employs a caching mechanism to avoid redundant evaluations. For a given set of independent variables, x, the optimizer requires the objective function, equality constraints, and inequality constraints to be provided separately. When working with complex models, these values are typically calculated all at once. If x hasn&#39;t changed from a previous evaluation, the caching system ensures that previously computed values are used, preventing unnecessary recalculations.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    problem : OptimizationProblem</span>
<span class="sd">        An instance of the optimization problem to be solved.</span>
<span class="sd">    library : str, optional</span>
<span class="sd">        The library to use for solving the optimization problem (default is &#39;scipy&#39;).</span>
<span class="sd">    method : str, optional</span>
<span class="sd">        The optimization method to use from the specified library (default is &#39;slsqp&#39;).</span>
<span class="sd">    tol : float, optional</span>
<span class="sd">        Tolerance for termination. The selected minimization algorithm sets some relevant solver-specific tolerance(s) equal to tol. The termination tolerances can be fine-tuned through the `options` dictionary. (default is 1e-5).</span>
<span class="sd">    max_iter : int, optional</span>
<span class="sd">        Maximum number of iterations for the optimizer (default is 100).</span>
<span class="sd">    options : dict, optional</span>
<span class="sd">        A dictionary of solver-specific options that prevails over &#39;tol&#39; and &#39;max_iter&#39;</span>
<span class="sd">    derivative_method : str, optional</span>
<span class="sd">        Method to use for derivative calculation (default is &#39;2-point&#39;).</span>
<span class="sd">    derivative_abs_step : float, optional</span>
<span class="sd">        Finite difference absolute step size to be used when the problem Jacobian is not provided. Defaults to 1e-6</span>
<span class="sd">    display : bool, optional</span>
<span class="sd">        If True, displays the convergence progress (default is True).</span>
<span class="sd">    plot : bool, optional</span>
<span class="sd">        If True, plots the convergence progress (default is False).</span>
<span class="sd">    plot_scale_objective : str, optional</span>
<span class="sd">        Specifies the scale of the objective function axis in the convergence plot (default is &#39;linear&#39;).</span>
<span class="sd">    plot_scale_constraints : str, optional</span>
<span class="sd">        Specifies the scale of the constraint violation axis in the convergence plot (default is &#39;linear&#39;).</span>
<span class="sd">    logger : logging.Logger, optional</span>
<span class="sd">        Logger object to which logging messages will be directed. Logging is disabled if `logger` is None.</span>
<span class="sd">    update_on : str, optional</span>
<span class="sd">        Specifies if the convergence report should be updated based on new function evaluations or gradient evaluations (default is &#39;gradient&#39;, alternative is &#39;function&#39;).</span>
<span class="sd">    callback_functions : list of callable or callable, optional</span>
<span class="sd">        Optional list of callback functions to pass to the solver.</span>
<span class="sd">    plot_improvement_only : bool, optional</span>
<span class="sd">        If True, plots only display iterations that improve the objective function value (useful for gradient-free optimizers) (default is False).</span>

<span class="sd">    Methods</span>
<span class="sd">    -------</span>
<span class="sd">    solve(x0):</span>
<span class="sd">        Solve the optimization problem using the specified initial guess `x0`.</span>
<span class="sd">    fitness(x):</span>
<span class="sd">        Evaluates the optimization problem objective function and constraints at a given point `x`.</span>
<span class="sd">    gradient(x):</span>
<span class="sd">        Evaluates the Jacobians of the optimization problem at a given point `x`.</span>
<span class="sd">    print_convergence_history():</span>
<span class="sd">        Print the final result and convergence history of the optimization problem.</span>
<span class="sd">    plot_convergence_history():</span>
<span class="sd">        Plot the convergence history of the optimization problem.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">problem</span><span class="p">,</span>
        <span class="n">library</span><span class="o">=</span><span class="s2">&quot;scipy&quot;</span><span class="p">,</span>
        <span class="n">method</span><span class="o">=</span><span class="s2">&quot;slsqp&quot;</span><span class="p">,</span>
        <span class="c1"># tolerance=1e-6,</span>
        <span class="c1"># max_iterations=100,</span>
        <span class="n">options</span><span class="o">=</span><span class="p">{},</span>
        <span class="n">derivative_method</span><span class="o">=</span><span class="s2">&quot;2-point&quot;</span><span class="p">,</span>
        <span class="n">derivative_abs_step</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">print_convergence</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">plot_convergence</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">plot_scale_objective</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span>
        <span class="n">plot_scale_constraints</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span>
        <span class="n">logger</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">update_on</span><span class="o">=</span><span class="s2">&quot;gradient&quot;</span><span class="p">,</span>
        <span class="n">callback_functions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">plot_improvement_only</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># Initialize class variables</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">problem</span> <span class="o">=</span> <span class="n">problem</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">display</span> <span class="o">=</span> <span class="n">print_convergence</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">plot</span> <span class="o">=</span> <span class="n">plot_convergence</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">plot_scale_objective</span> <span class="o">=</span> <span class="n">plot_scale_objective</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">plot_scale_constraints</span> <span class="o">=</span> <span class="n">plot_scale_constraints</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span> <span class="o">=</span> <span class="n">logger</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">library</span> <span class="o">=</span> <span class="n">library</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">=</span> <span class="n">method</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">derivative_method</span> <span class="o">=</span> <span class="n">derivative_method</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">derivative_abs_step</span> <span class="o">=</span> <span class="n">derivative_abs_step</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">callback_functions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_callback</span><span class="p">(</span><span class="n">callback_functions</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">callback_function_call_count</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">plot_improvement_only</span> <span class="o">=</span> <span class="n">plot_improvement_only</span>

        <span class="c1"># # Validate library and method</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_library_and_method</span><span class="p">()</span>

        <span class="c1"># Define options dictionary</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">options</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">options</span><span class="p">)</span> <span class="k">if</span> <span class="n">options</span> <span class="k">else</span> <span class="p">{}</span>
        <span class="c1"># self.options[&quot;tol&quot;] = tolerance</span>
        <span class="c1"># self.options[&quot;max_iter&quot;] = max_iterations</span>

        <span class="c1"># Check for logger validity</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">logger</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="p">,</span> <span class="n">logging</span><span class="o">.</span><span class="n">Logger</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;The provided logger is not a valid logging.Logger instance.&quot;</span>
                <span class="p">)</span>

        <span class="c1"># Check for valid display_on value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_on</span> <span class="o">=</span> <span class="n">update_on</span>
        <span class="k">if</span> <span class="n">update_on</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;function&quot;</span><span class="p">,</span> <span class="s2">&quot;gradient&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid value for &#39;update_on&#39;. It should be either &#39;function&#39; or &#39;gradient&#39;.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Rename number of constraints</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">N_eq</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">problem</span><span class="o">.</span><span class="n">get_nec</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">N_ineq</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">problem</span><span class="o">.</span><span class="n">get_nic</span><span class="p">()</span>

        <span class="c1"># Initialize variables for convergence report</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">f_final</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_final</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_last</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad_count</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">func_count</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">func_count_tot</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">success</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">message</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">solution_report</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">elapsed_time</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">include_solution_in_footer</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">convergence_history</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="p">[],</span>
            <span class="s2">&quot;grad_count&quot;</span><span class="p">:</span> <span class="p">[],</span>
            <span class="s2">&quot;func_count&quot;</span><span class="p">:</span> <span class="p">[],</span>
            <span class="s2">&quot;func_count_total&quot;</span><span class="p">:</span> <span class="p">[],</span>
            <span class="s2">&quot;objective_value&quot;</span><span class="p">:</span> <span class="p">[],</span>
            <span class="s2">&quot;constraint_violation&quot;</span><span class="p">:</span> <span class="p">[],</span>
            <span class="s2">&quot;norm_step&quot;</span><span class="p">:</span> <span class="p">[],</span>
        <span class="p">}</span>

        <span class="c1"># Initialize convergence plot</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">plot</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_plot_callback</span><span class="p">(</span><span class="n">initialize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Initialize dictionary for cached variables</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cache</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s2">&quot;f&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s2">&quot;c_eq&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s2">&quot;c_ineq&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s2">&quot;x_jac&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s2">&quot;f_jac&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s2">&quot;c_eq_jac&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s2">&quot;c_ineq_jac&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s2">&quot;fitness&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s2">&quot;gradient&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
        <span class="p">}</span>

    <span class="k">def</span> <span class="nf">_validate_library_and_method</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Check if the library is valid</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">library</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">VALID_LIBRARIES_AND_METHODS</span><span class="p">:</span>
            <span class="n">error_message</span> <span class="o">=</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Invalid optimization library &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">library</span><span class="si">}</span><span class="s2">&#39;. </span><span class="se">\n</span><span class="s2">Available libraries:</span><span class="se">\n</span><span class="s2">   - &quot;</span>
                <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">   - &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">VALID_LIBRARIES_AND_METHODS</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
                <span class="o">+</span> <span class="s2">&quot;.&quot;</span>
            <span class="p">)</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>

        <span class="c1"># Check if the method is valid for the selected library</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">VALID_LIBRARIES_AND_METHODS</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">library</span><span class="p">]:</span>
            <span class="n">error_message</span> <span class="o">=</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Invalid method &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">method</span><span class="si">}</span><span class="s2">&#39; for library &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">library</span><span class="si">}</span><span class="s2">&#39;. </span><span class="se">\n</span><span class="s2">Valid methods are:</span><span class="se">\n</span><span class="s2">   - &quot;</span>
                <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">   - &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">VALID_LIBRARIES_AND_METHODS</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">library</span><span class="p">])</span>
                <span class="o">+</span> <span class="s2">&quot;.&quot;</span>
            <span class="p">)</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_validate_callback</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">callback</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Validate the callback functions argument.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">callback</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="nb">callable</span><span class="p">(</span><span class="n">callback</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">callback</span><span class="p">]</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">callback</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="n">non_callable_items</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">callback</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">callable</span><span class="p">(</span><span class="n">item</span><span class="p">)]</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">non_callable_items</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">callback</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">error_msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;All elements in the callback list must be callable functions. Non-callable items: </span><span class="si">{</span><span class="n">non_callable_items</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">error_msg</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">error_msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;callback_func must be a function or a list of functions. Received type: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">callback</span><span class="p">)</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">callback</span><span class="si">}</span><span class="s2">)&quot;</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">error_msg</span><span class="p">)</span>

<div class="viewcode-block" id="OptimizationSolver.solve">
<a class="viewcode-back" href="../../../source/api/turboflow.pysolver_view.optimization.html#turboflow.pysolver_view.optimization.OptimizationSolver.solve">[docs]</a>
    <span class="k">def</span> <span class="nf">solve</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Solve the optimization problem using the specified library and solver.</span>

<span class="sd">        This method initializes the optimization process, manages the flow of the optimization,</span>
<span class="sd">        and handles the results, utilizing the solver from a specified library such as scipy or pygmo.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x0 : array-like, optional</span>
<span class="sd">            Initial guess for the solution of the optimization problem.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        x_final : array-like</span>
<span class="sd">            An array with the optimal vector of design variables</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Get start datetime</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">start_datetime</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s2">&quot;%Y-%m-</span><span class="si">%d</span><span class="s2">_%H-%M-%S&quot;</span><span class="p">)</span>

        <span class="c1"># Start timing with high-resolution timer</span>
        <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>

        <span class="c1"># Print report header</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_write_header</span><span class="p">()</span>

        <span class="c1"># Define new problem with anonymous methods (avoid problems when Pygmo creates a deep copy)</span>
        <span class="n">problem</span> <span class="o">=</span> <span class="n">_PygmoProblem</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="c1"># Fetch the solver function</span>
        <span class="n">lib_wrapper</span> <span class="o">=</span> <span class="n">OPTIMIZATION_LIBRARIES</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">library</span><span class="p">]</span>
        <span class="n">solution</span> <span class="o">=</span> <span class="n">lib_wrapper</span><span class="p">(</span><span class="n">problem</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">options</span><span class="p">)</span>

        <span class="c1"># Retrieve last solution (also works for gradient-free solvers when updating on gradient)</span>
        <span class="n">x_last</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_last</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_last</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">]</span>

        <span class="c1"># Save solution</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_final</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">x_last</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">f_final</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fitness</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_final</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">success</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">message</span> <span class="o">=</span> <span class="n">solution</span>

        <span class="c1"># Calculate elapsed time</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">elapsed_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>

        <span class="c1"># Print report footer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_print_convergence_progress</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_final</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_write_footer</span><span class="p">()</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_final</span></div>


<div class="viewcode-block" id="OptimizationSolver.fitness">
<a class="viewcode-back" href="../../../source/api/turboflow.pysolver_view.optimization.html#turboflow.pysolver_view.optimization.OptimizationSolver.fitness">[docs]</a>
    <span class="k">def</span> <span class="nf">fitness</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">called_from_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Evaluates the optimization problem values at a given point x.</span>

<span class="sd">        This method queries the `fitness` method of the OptimizationProblem class to</span>
<span class="sd">        compute the objective function value and constraint values. It first checks the cache</span>
<span class="sd">        to avoid redundant evaluations. If no matching cached result exists, it proceeds to</span>
<span class="sd">        evaluate the objective function and constraints.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : array-like</span>
<span class="sd">            Vector of independent variables (i.e., degrees of freedom).</span>
<span class="sd">        called_from_grad : bool, optional</span>
<span class="sd">            Flag used to indicate if the method is called during gradient evaluation.</span>
<span class="sd">            This helps in preventing redundant increments in evaluation counts during</span>
<span class="sd">            finite-differences gradient calculations. Default is False.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        fitness : numpy.ndarray</span>
<span class="sd">            A 1D array containing the objective function, equality constraints, and inequality constraints at `x`.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># If x hasn&#39;t changed, use cached values</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">array_equal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">]):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache</span><span class="p">[</span><span class="s2">&quot;fitness&quot;</span><span class="p">]</span>

        <span class="c1"># Increase total counter (includes finite differences)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">func_count_tot</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c1"># Evaluate objective function and constraints at once</span>
        <span class="n">fitness</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">problem</span><span class="o">.</span><span class="n">fitness</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Does not include finite differences</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">called_from_grad</span><span class="p">:</span>
            <span class="c1"># Update cached variabled</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cache</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span>  <span class="c1"># Needed for finite differences</span>
                    <span class="s2">&quot;f&quot;</span><span class="p">:</span> <span class="n">fitness</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                    <span class="s2">&quot;c_eq&quot;</span><span class="p">:</span> <span class="n">fitness</span><span class="p">[</span><span class="mi">1</span> <span class="p">:</span> <span class="mi">1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">N_eq</span><span class="p">],</span>
                    <span class="s2">&quot;c_ineq&quot;</span><span class="p">:</span> <span class="n">fitness</span><span class="p">[</span><span class="mi">1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">N_eq</span> <span class="p">:],</span>
                    <span class="s2">&quot;fitness&quot;</span><span class="p">:</span> <span class="n">fitness</span><span class="p">,</span>
                <span class="p">}</span>
            <span class="p">)</span>

            <span class="c1"># Increase minor iteration counter (line search)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">func_count</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="c1"># Update progress report</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_on</span> <span class="o">==</span> <span class="s2">&quot;function&quot;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_print_convergence_progress</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">fitness</span></div>


<div class="viewcode-block" id="OptimizationSolver.gradient">
<a class="viewcode-back" href="../../../source/api/turboflow.pysolver_view.optimization.html#turboflow.pysolver_view.optimization.OptimizationSolver.gradient">[docs]</a>
    <span class="k">def</span> <span class="nf">gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Evaluates the Jacobian matrix of the optimization problem at the given point x.</span>

<span class="sd">        This method utilizes the `gradient` method of the OptimizationProblem class if implemented.</span>
<span class="sd">        If the `gradient` method is not implemented, the Jacobian is approximated using forward finite differences.</span>

<span class="sd">        To prevent redundant calculations, cached results are checked first. If a matching</span>
<span class="sd">        cached result is found, it is returned; otherwise, a fresh calculation is performed.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : array-like</span>
<span class="sd">            Vector of independent variables (i.e., degrees of freedom).</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        numpy.ndarray</span>
<span class="sd">            A 2D array representing the Jacobian matrix of the optimization problem at `x`.</span>
<span class="sd">            The Jacobian matrix includes:</span>
<span class="sd">            - Gradient of the objective function</span>
<span class="sd">            - Jacobian of equality constraints</span>
<span class="sd">            - Jacobian of inequality constraints</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># If x hasn&#39;t changed, use cached values</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">array_equal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache</span><span class="p">[</span><span class="s2">&quot;x_jac&quot;</span><span class="p">]):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache</span><span class="p">[</span><span class="s2">&quot;gradient&quot;</span><span class="p">]</span>

        <span class="c1"># Use problem gradient method if it exists</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">problem</span><span class="p">,</span> <span class="s2">&quot;gradient&quot;</span><span class="p">):</span>
            <span class="n">grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">problem</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Fall back to finite differences</span>
            <span class="n">fun</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">fitness</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">called_from_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">grad</span> <span class="o">=</span> <span class="n">numerical_differentiation</span><span class="o">.</span><span class="n">approx_gradient</span><span class="p">(</span>
                <span class="n">fun</span><span class="p">,</span>
                <span class="n">x</span><span class="p">,</span>
                <span class="n">f0</span><span class="o">=</span><span class="n">fun</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
                <span class="n">method</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">derivative_method</span><span class="p">,</span>
                <span class="n">abs_step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">derivative_abs_step</span><span class="p">,</span>  <span class="c1">## TODO make sure it works when design variable takes value 0 * np.abs(x),</span>
            <span class="p">)</span>

        <span class="c1"># Reshape gradient for unconstrained problems</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>

        <span class="c1"># Update cache</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cache</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="s2">&quot;x_jac&quot;</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span>
                <span class="s2">&quot;f_jac&quot;</span><span class="p">:</span> <span class="n">grad</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:],</span>
                <span class="s2">&quot;c_eq_jac&quot;</span><span class="p">:</span> <span class="n">grad</span><span class="p">[</span><span class="mi">1</span> <span class="p">:</span> <span class="mi">1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">N_eq</span><span class="p">,</span> <span class="p">:],</span>
                <span class="s2">&quot;c_ineq_jac&quot;</span><span class="p">:</span> <span class="n">grad</span><span class="p">[</span><span class="mi">1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">N_eq</span> <span class="p">:,</span> <span class="p">:],</span>
                <span class="s2">&quot;gradient&quot;</span><span class="p">:</span> <span class="n">grad</span><span class="p">,</span>
            <span class="p">}</span>
        <span class="p">)</span>

        <span class="c1"># Update progress report</span>
        <span class="c1"># TODO check that the initial X is exact in cycle optimization</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad_count</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_on</span> <span class="o">==</span> <span class="s2">&quot;gradient&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_print_convergence_progress</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">grad</span></div>


    <span class="k">def</span> <span class="nf">_write_header</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Print a formatted header for the optimization report.</span>

<span class="sd">        This internal method is used to display a consistent header format at the</span>
<span class="sd">        beginning of the optimization process. The header includes columns for function</span>
<span class="sd">        evaluations, gradient evaluations, objective function value, constraint violations,</span>
<span class="sd">        and norm of the steps.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Define header text</span>
        <span class="n">initial_message</span> <span class="o">=</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot; Starting optimization process for </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">problem</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot; Optimization algorithm employed: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">method</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">header</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="s1">&#39;Grad-eval&#39;</span><span class="si">:</span><span class="s2">&gt;13</span><span class="si">}{</span><span class="s1">&#39;Func-eval&#39;</span><span class="si">:</span><span class="s2">&gt;13</span><span class="si">}{</span><span class="s1">&#39;Func-value&#39;</span><span class="si">:</span><span class="s2">&gt;16</span><span class="si">}{</span><span class="s1">&#39;Infeasibility&#39;</span><span class="si">:</span><span class="s2">&gt;18</span><span class="si">}{</span><span class="s1">&#39;Norm of step&#39;</span><span class="si">:</span><span class="s2">&gt;18</span><span class="si">}</span><span class="s2"> &quot;</span>
        <span class="n">separator</span> <span class="o">=</span> <span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">header</span><span class="p">)</span>
        <span class="n">lines_to_output</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">separator</span><span class="p">,</span>
            <span class="n">initial_message</span><span class="p">,</span>
            <span class="n">separator</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">header</span><span class="p">,</span>
            <span class="n">separator</span><span class="p">,</span>
        <span class="p">]</span>

        <span class="c1"># Display to stdout</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">display</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">lines_to_output</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>

        <span class="c1"># Write to log</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">lines_to_output</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>

        <span class="c1"># Store text in memory</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">solution_report</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">lines_to_output</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_print_convergence_progress</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Print the current optimization status and update convergence history.</span>

<span class="sd">        This method captures and prints the following metrics:</span>
<span class="sd">        - Number of gradient evaluations</span>
<span class="sd">        - Number of function evaluations</span>
<span class="sd">        - Objective function value</span>
<span class="sd">        - Maximum constraint violation</span>
<span class="sd">        - Norm of the update step</span>

<span class="sd">        The method also updates the stored convergence history for potential future analysis.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : array-like</span>
<span class="sd">            The current solution (i.e., vector of independent variable values)</span>

<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        The norm of the update step is calculated as the two-norm of the difference</span>
<span class="sd">        between the current and the last independent variables. Constraints violation is</span>
<span class="sd">        computed as the infinity norm of the active constraints.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Ensure fitness is computed at least once before printing</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache</span><span class="p">[</span><span class="s2">&quot;fitness&quot;</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">fitness</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Compute the norm of the last step</span>
        <span class="n">norm_step</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_last</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_last</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_last</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="c1"># Compute the maximun constraint violation</span>
        <span class="n">c_eq</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache</span><span class="p">[</span><span class="s2">&quot;c_eq&quot;</span><span class="p">]</span>
        <span class="n">c_ineq</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache</span><span class="p">[</span><span class="s2">&quot;c_ineq&quot;</span><span class="p">]</span>
        <span class="n">violation_all</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">c_eq</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">c_ineq</span><span class="p">,</span> <span class="mi">0</span><span class="p">)))</span>
        <span class="n">violation_max</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">violation_all</span><span class="p">))</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">violation_all</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mf">0.0</span>

        <span class="c1"># Store convergence status</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">convergence_history</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_last</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">convergence_history</span><span class="p">[</span><span class="s2">&quot;grad_count&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">grad_count</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">convergence_history</span><span class="p">[</span><span class="s2">&quot;func_count&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">func_count</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">convergence_history</span><span class="p">[</span><span class="s2">&quot;func_count_total&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">func_count_tot</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">convergence_history</span><span class="p">[</span><span class="s2">&quot;objective_value&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cache</span><span class="p">[</span><span class="s2">&quot;f&quot;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">convergence_history</span><span class="p">[</span><span class="s2">&quot;constraint_violation&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">violation_max</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">convergence_history</span><span class="p">[</span><span class="s2">&quot;norm_step&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">norm_step</span><span class="p">)</span>

        <span class="c1"># Current convergence message</span>
        <span class="n">status</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">grad_count</span><span class="si">:</span><span class="s2">13d</span><span class="si">}{</span><span class="bp">self</span><span class="o">.</span><span class="n">func_count</span><span class="si">:</span><span class="s2">13d</span><span class="si">}{</span><span class="bp">self</span><span class="o">.</span><span class="n">cache</span><span class="p">[</span><span class="s1">&#39;f&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">+16.3e</span><span class="si">}{</span><span class="n">violation_max</span><span class="si">:</span><span class="s2">+18.3e</span><span class="si">}{</span><span class="n">norm_step</span><span class="si">:</span><span class="s2">+18.3e</span><span class="si">}</span><span class="s2"> &quot;</span>

        <span class="c1"># Display to stdout</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">display</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">status</span><span class="p">)</span>

        <span class="c1"># Write to log</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="n">status</span><span class="p">)</span>

        <span class="c1"># Store text in memory</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">solution_report</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">status</span><span class="p">)</span>

        <span class="c1"># Refresh the plot with current values</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">plot</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_plot_callback</span><span class="p">()</span>

        <span class="c1"># Evaluate callback functions</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">callback_functions</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">callback_function_call_count</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">for</span> <span class="n">func</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">callback_functions</span><span class="p">:</span>
                <span class="n">func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">callback_function_call_count</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_write_footer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Print a formatted footer for the optimization report.</span>

<span class="sd">        This method displays the final optimization result, including the</span>
<span class="sd">        exit message, success status, objective function value, and decision variables.</span>

<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        The footer&#39;s structure is intended to match the header&#39;s style,</span>
<span class="sd">        providing a consistent look to the optimization report.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Define footer text</span>
        <span class="n">separator</span> <span class="o">=</span> <span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">header</span><span class="p">)</span>
        <span class="n">exit_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Exit message: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">message</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="n">success_status</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Success: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">success</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="n">time_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Solution time: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">elapsed_time</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> seconds&quot;</span>
        <span class="n">solution_header</span> <span class="o">=</span> <span class="s2">&quot;Solution:&quot;</span>
        <span class="n">solution_objective</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;   f  = </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">f_final</span><span class="si">:</span><span class="s2">+6e</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="n">solution_vars</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;   x</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> = </span><span class="si">{</span><span class="n">x</span><span class="si">:</span><span class="s2">+6e</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_final</span><span class="p">)]</span>
        <span class="n">lines_to_output</span> <span class="o">=</span> <span class="p">[</span><span class="n">separator</span><span class="p">,</span> <span class="n">success_status</span><span class="p">,</span> <span class="n">exit_message</span><span class="p">,</span> <span class="n">time_message</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">include_solution_in_footer</span><span class="p">:</span>
            <span class="n">lines_to_output</span> <span class="o">+=</span> <span class="p">[</span><span class="n">solution_header</span><span class="p">]</span>
            <span class="n">lines_to_output</span> <span class="o">+=</span> <span class="n">solution_objective</span>
            <span class="n">lines_to_output</span> <span class="o">+=</span> <span class="n">solution_vars</span>
        <span class="n">lines_to_output</span> <span class="o">+=</span> <span class="p">[</span><span class="n">separator</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">]</span>

        <span class="c1"># Display to stdout</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">display</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">lines_to_output</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>

        <span class="c1"># Write to log</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">lines_to_output</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>

        <span class="c1"># Store text in memory</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">solution_report</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">lines_to_output</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_plot_callback</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">initialize</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Callback function to dynamically update the convergence progress plot.</span>

<span class="sd">        This method initializes a matplotlib plot on the first iteration and updates</span>
<span class="sd">        the data for each subsequent iteration. The plot showcases the evolution of</span>
<span class="sd">        the objective function and the constraint violation with respect to the</span>
<span class="sd">        number of iterations.</span>

<span class="sd">        The left y-axis depicts the objective function values, while the right y-axis</span>
<span class="sd">        showcases the constraint violation values. The x-axis represents the number</span>
<span class="sd">        of iterations. Both lines are updated and redrawn dynamically as iterations progress.</span>

<span class="sd">        Note:</span>
<span class="sd">            This is an internal method, meant to be called within the optimization process.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Initialize figure before first iteration</span>
        <span class="k">if</span> <span class="n">initialize</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">fig</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ax_1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">obj_line_1</span><span class="p">,)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ax_1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
                <span class="p">[],</span> <span class="p">[],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;#0072BD&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Objective function&quot;</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ax_1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Number of iterations&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ax_1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Objective function&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ax_1</span><span class="o">.</span><span class="n">set_yscale</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">plot_scale_objective</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ax_1</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_major_locator</span><span class="p">(</span>
                <span class="n">MaxNLocator</span><span class="p">(</span><span class="n">integer</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="p">)</span>  <span class="c1"># Integer ticks</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">N_eq</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">N_ineq</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">ax_2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ax_1</span><span class="o">.</span><span class="n">twinx</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">ax_2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Constraint violation&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">ax_2</span><span class="o">.</span><span class="n">set_yscale</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">plot_scale_constraints</span><span class="p">)</span>
                <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">obj_line_2</span><span class="p">,)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ax_2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
                    <span class="p">[],</span> <span class="p">[],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;#D95319&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Constraint violation&quot;</span>
                <span class="p">)</span>
                <span class="n">lines</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">obj_line_1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">obj_line_2</span><span class="p">]</span>
                <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">l</span><span class="o">.</span><span class="n">get_label</span><span class="p">()</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">ax_2</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">lines</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">ax_1</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">(</span><span class="n">pad</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Update plot data with current values</span>
        <span class="n">iteration</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">convergence_history</span><span class="p">[</span><span class="s2">&quot;func_count&quot;</span><span class="p">]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_on</span> <span class="o">==</span> <span class="s2">&quot;function&quot;</span>
            <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">convergence_history</span><span class="p">[</span><span class="s2">&quot;grad_count&quot;</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="n">objective_function</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convergence_history</span><span class="p">[</span><span class="s2">&quot;objective_value&quot;</span><span class="p">]</span>
        <span class="n">constraint_violation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convergence_history</span><span class="p">[</span><span class="s2">&quot;constraint_violation&quot;</span><span class="p">]</span>

        <span class="c1"># Iterate through the objective_function values to create the new series</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">plot_improvement_only</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">objective_function</span><span class="p">)):</span>
                <span class="k">if</span> <span class="n">objective_function</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">objective_function</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]:</span>
                    <span class="n">objective_function</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">objective_function</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>

        <span class="c1"># Update graphic objects data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">obj_line_1</span><span class="o">.</span><span class="n">set_xdata</span><span class="p">(</span><span class="n">iteration</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">obj_line_1</span><span class="o">.</span><span class="n">set_ydata</span><span class="p">(</span><span class="n">objective_function</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">N_eq</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">N_ineq</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">obj_line_2</span><span class="o">.</span><span class="n">set_xdata</span><span class="p">(</span><span class="n">iteration</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">obj_line_2</span><span class="o">.</span><span class="n">set_ydata</span><span class="p">(</span><span class="n">constraint_violation</span><span class="p">)</span>

        <span class="c1"># Adjust the plot limits</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ax_1</span><span class="o">.</span><span class="n">relim</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ax_1</span><span class="o">.</span><span class="n">autoscale_view</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">N_eq</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">N_ineq</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ax_2</span><span class="o">.</span><span class="n">relim</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ax_2</span><span class="o">.</span><span class="n">autoscale_view</span><span class="p">()</span>

        <span class="c1"># Redraw the plot</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">draw</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">pause</span><span class="p">(</span><span class="mf">0.01</span><span class="p">)</span>  <span class="c1"># small pause to allow for update</span>

<div class="viewcode-block" id="OptimizationSolver.print_convergence_history">
<a class="viewcode-back" href="../../../source/api/turboflow.pysolver_view.optimization.html#turboflow.pysolver_view.optimization.OptimizationSolver.print_convergence_history">[docs]</a>
    <span class="k">def</span> <span class="nf">print_convergence_history</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">savefile</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">filename</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_dir</span><span class="o">=</span><span class="s2">&quot;output&quot;</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Print the convergence history of the problem.</span>

<span class="sd">        The convergence history includes:</span>
<span class="sd">            - Number of function evaluations</span>
<span class="sd">            - Number of gradient evaluations</span>
<span class="sd">            - Objective function value</span>
<span class="sd">            - Maximum constraint violation</span>
<span class="sd">            - Two-norm of the update step</span>

<span class="sd">        The method provides a detailed report on:</span>
<span class="sd">            - Exit message</span>
<span class="sd">            - Success status</span>
<span class="sd">            - Execution time</span>

<span class="sd">        This method should be called only after the optimization problem has been solved, as it relies on data generated by the solving process.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        savefile : bool, optional</span>
<span class="sd">            If True, the convergence history will be saved to a file, otherwise printed to standard output. Default is False.</span>
<span class="sd">        filename : str, optional</span>
<span class="sd">            The name of the file to save the convergence history. If not specified, the filename is automatically generated</span>
<span class="sd">            using the problem name and the start datetime. The file extension is not required.</span>
<span class="sd">        output_dir : str, optional</span>
<span class="sd">            The directory where the plot file will be saved if savefile is True. Default is &quot;output&quot;.</span>

<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        ValueError</span>
<span class="sd">            If this method is called before the problem has been solved.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_final</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">savefile</span><span class="p">:</span>
                <span class="c1"># Create output directory if it does not exist</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">output_dir</span><span class="p">):</span>
                    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">output_dir</span><span class="p">)</span>

                <span class="c1"># Give a name to the file if it is not specified</span>
                <span class="k">if</span> <span class="n">filename</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">filename</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;convergence_</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">problem</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">start_datetime</span><span class="si">}</span><span class="s2">.txt&quot;</span>

                <span class="c1"># Write report to file</span>
                <span class="n">fullfile</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_dir</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
                <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">fullfile</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">solution_report</span><span class="p">))</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">solution_report</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;This method can only be used after invoking the &#39;solve()&#39; method.&quot;</span>
            <span class="p">)</span></div>


<div class="viewcode-block" id="OptimizationSolver.plot_convergence_history">
<a class="viewcode-back" href="../../../source/api/turboflow.pysolver_view.optimization.html#turboflow.pysolver_view.optimization.OptimizationSolver.plot_convergence_history">[docs]</a>
    <span class="k">def</span> <span class="nf">plot_convergence_history</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">savefile</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">filename</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_dir</span><span class="o">=</span><span class="s2">&quot;output&quot;</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Plot the convergence history of the problem.</span>

<span class="sd">        This method plots the optimization progress against the number of iterations:</span>
<span class="sd">            - Objective function value (left y-axis)</span>
<span class="sd">            - Maximum constraint violation (right y-axis)</span>

<span class="sd">        The constraint violation is only displayed if the problem has nonlinear constraints</span>

<span class="sd">        This method should be called only after the optimization problem has been solved, as it relies on data generated by the solving process.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        savefile : bool, optional</span>
<span class="sd">            If True, the plot is saved to a file instead of being displayed. Default is False.</span>
<span class="sd">        filename : str, optional</span>
<span class="sd">            The name of the file to save the plot to. If not specified, the filename is automatically generated</span>
<span class="sd">            using the problem name and the start datetime. The file extension is not required.</span>
<span class="sd">        output_dir : str, optional</span>
<span class="sd">            The directory where the plot file will be saved if savefile is True. Default is &quot;output&quot;.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        matplotlib.figure.Figure</span>
<span class="sd">            The Matplotlib figure object for the plot. This can be used for further customization or display.</span>

<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        ValueError</span>
<span class="sd">            If this method is called before the problem has been solved.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_final</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_plot_callback</span><span class="p">(</span><span class="n">initialize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;This method can only be used after invoking the &#39;solve()&#39; method.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">savefile</span><span class="p">:</span>
            <span class="c1"># Create output directory if it does not exist</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">output_dir</span><span class="p">):</span>
                <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">output_dir</span><span class="p">)</span>

            <span class="c1"># Give a name to the file if it is not specified</span>
            <span class="k">if</span> <span class="n">filename</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">filename</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;convergence_</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">problem</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">start_datetime</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>

            <span class="c1"># Save plots</span>
            <span class="n">fullfile</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_dir</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
            <span class="n">savefig_in_formats</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fig</span><span class="p">,</span> <span class="n">fullfile</span><span class="p">,</span> <span class="n">formats</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;.png&quot;</span><span class="p">,</span> <span class="s2">&quot;.svg&quot;</span><span class="p">])</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fig</span></div>
</div>



<div class="viewcode-block" id="OptimizationProblem">
<a class="viewcode-back" href="../../../source/api/turboflow.pysolver_view.optimization.html#turboflow.pysolver_view.optimization.OptimizationProblem">[docs]</a>
<span class="k">class</span> <span class="nc">OptimizationProblem</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Abstract base class for optimization problems.</span>

<span class="sd">    Derived optimization problem objects must implement the following methods:</span>

<span class="sd">    - `fitness`: Evaluate the objective function and constraints for a given set of decision variables.</span>
<span class="sd">    - `get_bounds`: Get the bounds for each decision variable.</span>
<span class="sd">    - `get_neq`: Return the number of equality constraints associated with the problem.</span>
<span class="sd">    - `get_nineq`: Return the number of inequality constraints associated with the problem.</span>

<span class="sd">    Additionally, specific problem classes can define the `gradient` method to compute the Jacobians. If this method is not present in the derived class, the solver will revert to using forward finite differences for Jacobian calculations.</span>

<span class="sd">    Methods</span>
<span class="sd">    -------</span>
<span class="sd">    fitness(x)</span>
<span class="sd">        Evaluate the objective function and constraints for a given set of decision variables.</span>
<span class="sd">    get_bounds()</span>
<span class="sd">        Get the bounds for each decision variable.</span>
<span class="sd">    get_neq()</span>
<span class="sd">        Return the number of equality constraints associated with the problem.</span>
<span class="sd">    get_nineq()</span>
<span class="sd">        Return the number of inequality constraints associated with the problem.</span>

<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="OptimizationProblem.fitness">
<a class="viewcode-back" href="../../../source/api/turboflow.pysolver_view.optimization.html#turboflow.pysolver_view.optimization.OptimizationProblem.fitness">[docs]</a>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">fitness</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Evaluate the objective function and constraints for given decision variables.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : array-like</span>
<span class="sd">            Vector of independent variables (i.e., degrees of freedom).</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        array_like</span>
<span class="sd">            Vector containing the objective function, equality constraints, and inequality constraints.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">pass</span></div>


<div class="viewcode-block" id="OptimizationProblem.get_bounds">
<a class="viewcode-back" href="../../../source/api/turboflow.pysolver_view.optimization.html#turboflow.pysolver_view.optimization.OptimizationProblem.get_bounds">[docs]</a>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">get_bounds</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the bounds for each decision variable (Pygmo format)</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        bounds : tuple of lists</span>
<span class="sd">            A tuple of two items where the first item is the list of lower bounds and the second</span>
<span class="sd">            item of the list of upper bounds for the vector of decision variables. For example,</span>
<span class="sd">            ([-2 -1], [2, 1]) indicates that the first decision variable has bounds between</span>
<span class="sd">            -2 and 2, and the second has bounds between -1 and 1.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">pass</span></div>


<div class="viewcode-block" id="OptimizationProblem.get_nec">
<a class="viewcode-back" href="../../../source/api/turboflow.pysolver_view.optimization.html#turboflow.pysolver_view.optimization.OptimizationProblem.get_nec">[docs]</a>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">get_nec</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return the number of equality constraints associated with the problem.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        neq : int</span>
<span class="sd">            Number of equality constraints.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">pass</span></div>


<div class="viewcode-block" id="OptimizationProblem.get_nic">
<a class="viewcode-back" href="../../../source/api/turboflow.pysolver_view.optimization.html#turboflow.pysolver_view.optimization.OptimizationProblem.get_nic">[docs]</a>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">get_nic</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return the number of inequality constraints associated with the problem.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        nineq : int</span>
<span class="sd">            Number of inequality constraints.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">pass</span></div>
</div>



<div class="viewcode-block" id="count_constraints">
<a class="viewcode-back" href="../../../source/api/turboflow.pysolver_view.optimization.html#turboflow.pysolver_view.optimization.count_constraints">[docs]</a>
<span class="k">def</span> <span class="nf">count_constraints</span><span class="p">(</span><span class="n">var</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Retrieve the number of constraints based on the provided input.</span>

<span class="sd">    This function returns the count of constraints based on the nature of the</span>
<span class="sd">    input:</span>

<span class="sd">    - `None` returns 0</span>
<span class="sd">    - Scalar values return 1</span>
<span class="sd">    - Array-like structures return their length</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    var : None, scalar, or array-like (list, tuple, np.ndarray)</span>
<span class="sd">        The input representing the constraint(s). This can be `None`, a scalar value,</span>
<span class="sd">        or an array-like structure containing multiple constraints.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    int</span>
<span class="sd">        The number of constraints:</span>

<span class="sd">        - 0 for `None`</span>
<span class="sd">        - 1 for scalar values</span>
<span class="sd">        - Length of the array-like for array-like inputs</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; count_constraints(None)</span>
<span class="sd">    0</span>

<span class="sd">    &gt;&gt;&gt; count_constraints(5.0)</span>
<span class="sd">    1</span>

<span class="sd">    &gt;&gt;&gt; count_constraints([1.0, 2.0, 3.0])</span>
<span class="sd">    3</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># If constraint is None</span>
    <span class="k">if</span> <span class="n">var</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>
    <span class="c1"># If constraint is a scalar (assuming it&#39;s numeric)</span>
    <span class="k">elif</span> <span class="n">np</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">var</span><span class="p">):</span>
        <span class="k">return</span> <span class="mi">1</span>
    <span class="c1"># If constraint is array-like</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">var</span><span class="p">)</span></div>



<div class="viewcode-block" id="combine_objective_and_constraints">
<a class="viewcode-back" href="../../../source/api/turboflow.pysolver_view.optimization.html#turboflow.pysolver_view.optimization.combine_objective_and_constraints">[docs]</a>
<span class="k">def</span> <span class="nf">combine_objective_and_constraints</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">c_eq</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">c_ineq</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Combine an objective function with its associated equality and inequality constraints.</span>

<span class="sd">    This function takes in an objective function value, a set of equality constraints,</span>
<span class="sd">    and a set of inequality constraints. It then returns a combined Numpy array of</span>
<span class="sd">    these values. The constraints can be given as a list, tuple, numpy array, or as</span>
<span class="sd">    individual values.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    f : float</span>
<span class="sd">        The value of the objective function.</span>
<span class="sd">    c_eq : float, list, tuple, np.ndarray, or None</span>
<span class="sd">        The equality constraint(s). This can be a single value or a collection of values.</span>
<span class="sd">        If `None`, no equality constraints will be added.</span>
<span class="sd">    c_ineq : float, list, tuple, np.ndarray, or None</span>
<span class="sd">        The inequality constraint(s). This can be a single value or a collection of values.</span>
<span class="sd">        If `None`, no inequality constraints will be added.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    np.ndarray</span>
<span class="sd">        A numpy array consisting of the objective function value followed by equality and</span>
<span class="sd">        inequality constraints.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; combine_objective_and_constraints(1.0, [0.5, 0.6], [0.7, 0.8])</span>
<span class="sd">    array([1. , 0.5, 0.6, 0.7, 0.8])</span>

<span class="sd">    &gt;&gt;&gt; combine_objective_and_constraints(1.0, 0.5, 0.7)</span>
<span class="sd">    array([1. , 0.5, 0.7])</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Validate objective function value</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">f</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Objective function value &#39;f&#39; must be a scalar or single-element array.&quot;</span>
            <span class="p">)</span>
        <span class="n">f</span> <span class="o">=</span> <span class="n">f</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># Unwrap the single element to ensure it&#39;s treated as a scalar</span>

    <span class="c1"># Add objective function</span>
    <span class="n">combined_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span><span class="p">]</span>

    <span class="c1"># Add equality constraints</span>
    <span class="k">if</span> <span class="n">c_eq</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">c_eq</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)):</span>
            <span class="n">combined_list</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">c_eq</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">combined_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">c_eq</span><span class="p">)</span>

    <span class="c1"># Add inequality constraints</span>
    <span class="k">if</span> <span class="n">c_ineq</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">c_ineq</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)):</span>
            <span class="n">combined_list</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">c_ineq</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">combined_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">c_ineq</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">combined_list</span><span class="p">)</span></div>



<span class="k">class</span> <span class="nc">_PygmoProblem</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A wrapper class for optimization problems to be compatible with Pygmo&#39;s need for deep-copiable problems.</span>
<span class="sd">    This class uses anonymous functions (lambda) to prevent issues with deep copying complex objects,</span>
<span class="sd">    (like Coolprop&#39;s AbstractState objects) which are not deep-copiable.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">wrapped_problem</span><span class="p">):</span>
        <span class="c1"># Pygmo requires a flattened Jacobian for gradients, unlike SciPy&#39;s two-dimensional array.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fitness</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">wrapped_problem</span><span class="o">.</span><span class="n">fitness</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">wrapped_problem</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

        <span class="c1"># Directly link bounds and constraint counts from the original problem.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">get_bounds</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">wrapped_problem</span><span class="o">.</span><span class="n">problem</span><span class="o">.</span><span class="n">get_bounds</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">get_nec</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">wrapped_problem</span><span class="o">.</span><span class="n">problem</span><span class="o">.</span><span class="n">get_nec</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">get_nic</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">wrapped_problem</span><span class="o">.</span><span class="n">problem</span><span class="o">.</span><span class="n">get_nic</span><span class="p">()</span>

        <span class="c1"># If the original problem defines Hessians, provide them as well.</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">wrapped_problem</span><span class="o">.</span><span class="n">problem</span><span class="p">,</span> <span class="s2">&quot;hessians&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">hessians</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">wrapped_problem</span><span class="o">.</span><span class="n">problem</span><span class="o">.</span><span class="n">hessians</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Define anonymous functions for objective and constraints with their Jacobians.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">wrapped_problem</span><span class="o">.</span><span class="n">fitness</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c_eq</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">wrapped_problem</span><span class="o">.</span><span class="n">fitness</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">1</span> <span class="p">:</span> <span class="mi">1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_nec</span><span class="p">()]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c_ineq</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">wrapped_problem</span><span class="o">.</span><span class="n">fitness</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_nec</span><span class="p">()</span> <span class="p">:]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">f_jac</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">wrapped_problem</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c_eq_jac</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">wrapped_problem</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">1</span> <span class="p">:</span> <span class="mi">1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_nec</span><span class="p">(),</span> <span class="p">:]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c_ineq_jac</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">wrapped_problem</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_nec</span><span class="p">()</span> <span class="p">:,</span> <span class="p">:]</span>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Lasse Borg Anderson and Roberto Agromayor.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>